{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfdad8ff",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue;border: 2px solid gray;\">\n",
    "    <h2 style =\"text-align:center; padding-top:5px;\"> CS 101 - Foundation of Data Science and Engineering  </h2><br>\n",
    "    <p style=\"text-align:center;padding:5px; fontt-size:14px\"><b> PSET-5 - Exploratory Data Analysis (100 pts)<b></p> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aca83fb",
   "metadata": {},
   "source": [
    "# This is an individual assignment. No collaboration is allowed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7ec799",
   "metadata": {},
   "source": [
    "### Note: \n",
    "You are allowed to use all Python built-in functions and other Import features covered in class. Ensure your code is organized, well-commented, and follows the best practices we discussed. Remember, the key is not just to write a working program but to produce a solution that follows SPECS, is easy to debug, and is easy to maintain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41163e5e",
   "metadata": {},
   "source": [
    "### Assignment Question List:\n",
    " \n",
    "**Question 1 : Data Extraction and cleaning (1 pts)**\n",
    "\n",
    "**Question 2 : Column Cleaning Function (15 pts)**\n",
    "\n",
    "**Question 3 : Data Cleaning (9 pts)**\n",
    "\n",
    "**Question 4 : Data Aggregation (30 pts)**\n",
    "\n",
    "**Question 5 : Merging Data (10 pts)**\n",
    "\n",
    "**Question 6 : Filtering Data (30 pts)**\n",
    "\n",
    "**Coding Style : (5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab336b0",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "**You are tasked with performing detailed exploratory data analysis on various system datasets: CPU, Disk, and Memory. Utilize Python functions to streamline data extraction, cleaning, and aggregation.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32b9bd",
   "metadata": {},
   "source": [
    "### Datasets:\n",
    "\n",
    "1. **CPU Dataset: `cpu.csv`**\n",
    "   - **Columns:** 'Image', 'PID', 'Description', 'Status', 'Threads', 'CPU', 'Average CPU'\n",
    "   <br><br>\n",
    "2. **Disk Dataset: `disk.csv`**\n",
    "   - **Columns:** 'Image', 'PID', 'Model', 'Read Byte Per Second', 'Write Byte Per Second', 'Delay', 'Total Byte Per Second'\n",
    "    <br><br>\n",
    "3. **Memory Dataset: `memory.csv`**\n",
    "   - **Columns:** 'Image', 'PID', 'Hard Faults/sec', 'Commit KB', 'Working Set KB', 'Shareable KB', 'Private KB'\n",
    "    <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89055f",
   "metadata": {},
   "source": [
    "# Your Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b1d8f8",
   "metadata": {},
   "source": [
    "### 1. Data Extraction (1 pts):\n",
    "\n",
    "a. **Load `cpu.csv` into a Pandas DataFrame named `df_cpu`. Extract only these columns:** 'Image', 'PID', 'Description', 'Status', 'Threads', 'Average CPU'\n",
    "\n",
    "b. **Load `disk.csv` into a DataFrame named `df_disk`. Extract only these columns:** 'Image', 'PID', 'Total Byte Per Second'\n",
    "\n",
    "c. **Load `memory.csv` into a DataFrame named `df_memory`. Extract only these columns:** 'Image', 'PID', 'Working Set KB'\n",
    "\n",
    "d. **For each of the above data frame show the shape, and info(example : df_cpu.info())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "file_path = \"cpu.csv\"\n",
    "df_cpu = pd.read_csv(file_path)\n",
    "\n",
    "# Extract the specified columns (Image, PID, Total Byte\n",
    "df_cpu = df_cpu[['Image', 'PID', 'Description', 'Status', 'Threads', 'Average CPU']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75e715-b3d1-4fdf-b8f9-d67d2eae2c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disk.csv into a DataFrame and extract specified columns (Image, PID, Total Byte Per Second)\n",
    "df_disk = pd.read_csv(\"disk.csv\")\n",
    "df_disk = df_disk[['Image', 'PID', 'Total Byte Per Second']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c0dee-133a-470e-a204-2ad5dd5b467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load memory.csv into a DataFrame and extract specified columns (Image, PID, Working Set KB)\n",
    "df_memory = pd.read_csv(\"memory.csv\")\n",
    "df_memory = df_memory[['Image', 'PID', 'Working Set KB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca358fe-8c77-4745-b4c0-832f5721c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display shape and info for each DataFrame\n",
    "print(\"CPU DataFrame:\")\n",
    "print(df_cpu.shape)\n",
    "df_cpu.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Disk DataFrame:\")\n",
    "print(df_disk.shape)\n",
    "df_disk.info()\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Memory DataFrame:\")\n",
    "print(df_memory.shape)\n",
    "df_memory.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dec275",
   "metadata": {},
   "source": [
    "### 2. Column Cleaning Function (15 pts):\n",
    "\n",
    "a. **Write a Python function, `CleanColumnHeading(dfx)`, to clean the column headers of any Pandas dataframe. The function should be dynamic enough to be able to process any datasets. The function should:**\n",
    "\n",
    "   i. Convert all column names to lowercase.\n",
    "   \n",
    "   ii. Replace spaces in column names with underscores `_`.\n",
    "\n",
    "   iii. Apply the `CleanColumnHeading` function to `df_cpu`, `df_disk`, and `df_memory`.\n",
    "\n",
    "   iv. Show examples of the changes.  \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Function header:\n",
    "```python\n",
    "def CleanColumnHeading(dfx):\n",
    "    # Your code to convert all column names to lowercase\n",
    "    # Your code to change all spaces in column names to underscores _\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean column names\n",
    "def CleanColumnHeading(dfx):\n",
    "    \"\"\"\n",
    "    Cleans the column names:\n",
    "    - Converts all column names to lowercase.\n",
    "    - Replaces spaces with underscores.\n",
    "    \"\"\"\n",
    "    dfx.columns = dfx.columns.str.lower().str.replace(' ', '_')\n",
    "    return dfx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0392dea-7102-4a1e-baf7-afa87d823ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_cpu = pd.read_csv(\"cpu.csv\")\n",
    "df_disk = pd.read_csv(\"disk.csv\")\n",
    "df_memory = pd.read_csv(\"memory.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db8ed38-5f06-4364-a146-825c01ea270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each data frame\n",
    "df_cpu = CleanColumnHeading(df_cpu)\n",
    "df_disk = CleanColumnHeading(df_disk)\n",
    "df_memory = CleanColumnHeading(df_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd238f0-8152-4731-a8ba-503f5c59b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of changes\n",
    "print(\"CPU DataFrame columns:\", df_cpu.columns)\n",
    "print(\"Disk DataFrame columns:\", df_disk.columns)\n",
    "print(\"Memory DataFrame columns:\", df_memory.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e86fa-c8c8-4e50-9f75-517491d29e68",
   "metadata": {},
   "source": [
    "### 3. Data cleaning (9 pts)\n",
    "**Examine the columns 'threads', 'total_byte_per_second', 'working_set_kb' from the dataframes df_cpu, df_disk, and df_memory. We are going to work with these columns Question 4-6. Ensure that they have the correct data type, fix the values if required so. If there are invalid values drop them. At the end of this step you should not have any invalid values, and the correct data type set for the 3 columns. You are not required to do data cleaning on other columns. If you choose to do so, please ensure that no rows are dropped while cleaning these columns** \n",
    "\n",
    "**Hint: values such as 71,807 are not invalid. They are simply string representation of the number 71807. Fix it so they are stored as 71807, and the column datatype should either be a float or int.** \n",
    "\n",
    "\n",
    "**For each of the columns modified , show example rows of what was modified. Also call the info() method on each of the dataframes.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1a8e5-9532-4624-a143-2ff579b99909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean numeric columns\n",
    "def clean_numeric_column(dfx, column_name):\n",
    "    \"\"\"\n",
    "    Converts a column to numeric by:\n",
    "    - Removing commas (e.g., \"71,807\" â†’ \"71807\")\n",
    "    - Converting to float or int\n",
    "    - Dropping invalid values\n",
    "    \"\"\"\n",
    "    dfx[column_name] = dfx[column_name].astype(str).str.replace(',', '')  # Remove commas\n",
    "    dfx[column_name] = pd.to_numeric(dfx[column_name], errors='coerce')  # Convert to number\n",
    "    before_drop = len(dfx)\n",
    "    dfx = dfx.dropna(subset=[column_name])  # Drop rows with invalid values\n",
    "    after_drop = len(dfx)\n",
    "\n",
    "  # Show examples of modified rows (if any rows were dropped)\n",
    "    if before_drop != after_drop:\n",
    "        print(f\"Modified rows in {column_name} (before dropping NaN values):\")\n",
    "        print(dfx[[column_name]].head())\n",
    "    \n",
    "    return dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50664004-97f4-4f5e-b47b-b4c1d75cc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'threads' column in df_cpu\n",
    "df_cpu = clean_numeric_column(df_cpu, 'threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8418df3f-bd4c-4179-ae8b-3b705164902a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'total_byte_per_second' column in df_disk\n",
    "df_disk = clean_numeric_column(df_disk, 'total_byte_per_second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e988c47-5362-4b97-8ae3-eb77936239c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean 'working_set_kb' column in df_memory\n",
    "df_memory = clean_numeric_column(df_memory, 'working_set_kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0643c610-ea2f-462e-ba41-bf0a16b20428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display updated DataFrame info\n",
    "print(\"\\nCPU DataFrame Info:\")\n",
    "df_cpu.info()\n",
    "\n",
    "print(\"\\nDisk DataFrame Info:\")\n",
    "df_disk.info()\n",
    "\n",
    "print(\"\\nMemory DataFrame Info:\")\n",
    "df_memory.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df9d5d",
   "metadata": {},
   "source": [
    "### 4. Data Aggregation (30 pts):\n",
    "\n",
    "a. **Develop an `aggregate_data(dfx)` function that:**\n",
    "\n",
    "   - Accepts one of the dataframes (`df_cpu`, `df_disk`, or `df_memory`).\n",
    "   - Dynamically identifies the dataset based on its columns.\n",
    "   - Aggregates the data based on the specific dataset requirements provided below and returns an aggregated dataframe. You can name this new aggregated dataframe whatever name works (examples given).\n",
    "\n",
    "b. **Aggregation Requirements:**\n",
    "\n",
    "   i. For `df_cpu`:\n",
    "   \n",
    "          1. Group by 'image' and aggregate the 'threads' column. Use the sum() function for aggregation.\n",
    "          2. The resultant dataframe should contain: 'image_name', 'process_qty', 'threads_sum'.\n",
    "          3. Reset the index after aggregation.\n",
    "          4. Assign the new aggregated dataframe values to df_cpu_sum.\n",
    "   \n",
    "   ii. For `df_disk`:\n",
    "   \n",
    "          1. Group by 'image' and aggregate the 'total_byte_per_second' column.Use the sum() function for aggregation.\n",
    "          2. The resultant dataframe should contain: 'image_name', 'process_qty', 'total_byte_sum'.\n",
    "          3. Reset the index after aggregation.\n",
    "          4. Assign the new aggregated dataframe values to df_disk_sum.\n",
    "   \n",
    "   iii. For `df_memory`:\n",
    "   \n",
    "          1. Group by 'image' and aggregate the 'working_set_kb' column. Use the sum() function for aggregation.\n",
    "          2. The resultant dataframe should contain: 'image_name', 'process_qty', 'working_set_sum'.\n",
    "          3. Reset the index after aggregation.\n",
    "          4. Assign the new aggregated dataframe values to df_memory_sum.\n",
    "   \n",
    "   **Note: 'process_qty' is defined by grouping by 'image' and determining the number of times a particular image name occurs. Use the size() function for aggregation**\n",
    "          \n",
    "c. **Run `aggregate_data()` for each of your dataframes and save them into new dataframes. Print the head(10) and tail(10) for each of the new dataframes.**\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Function header:\n",
    "```python\n",
    "def aggregate_data(dfx):\n",
    "    # Your code aggregates the data based on the requirements provided.\n",
    "    return df_combo  # Name this resultant dataframe whatever name that works\n",
    "```\n",
    "Calling function and assigning the resultant dataframe to `df_cpu_sum`, `df_disk_sum`, `df_memory_sum`:\n",
    "```python\n",
    "df_cpu_sum = aggregate_data(df_cpu)\n",
    "df_disk_sum = aggregate_data(df_disk)\n",
    "df_memory_sum = aggregate_data(df_memory)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bafe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate data dynamically based on dataset type\n",
    "def aggregate_data(dfx):\n",
    "    \"\"\"\n",
    "    Aggregates the given dataframe based on its dataset type:\n",
    "    - df_cpu: Groups by 'image', sums 'threads', and counts occurrences.\n",
    "    - df_disk: Groups by 'image', sums 'total_byte_per_second', and counts occurrences.\n",
    "    - df_memory: Groups by 'image', sums 'working_set_kb', and counts occurrences.\n",
    "    \"\"\"\n",
    "    # Identify dataset type based on columns\n",
    "    if 'threads' in dfx.columns:\n",
    "        df_combo = dfx.groupby('image').agg(\n",
    "            process_qty=('image', 'size'),\n",
    "            threads_sum=('threads', 'sum')\n",
    "        ).reset_index()\n",
    "        df_combo.rename(columns={'image': 'image_name'}, inplace=True)\n",
    "    \n",
    "    elif 'total_byte_per_second' in dfx.columns:\n",
    "        df_combo = dfx.groupby('image').agg(\n",
    "            process_qty=('image', 'size'),\n",
    "            total_byte_sum=('total_byte_per_second', 'sum')\n",
    "        ).reset_index()\n",
    "        df_combo.rename(columns={'image': 'image_name'}, inplace=True)\n",
    "    \n",
    "    elif 'working_set_kb' in dfx.columns:\n",
    "        df_combo = dfx.groupby('image').agg(\n",
    "            process_qty=('image', 'size'),\n",
    "            working_set_sum=('working_set_kb', 'sum')\n",
    "        ).reset_index()\n",
    "        df_combo.rename(columns={'image': 'image_name'}, inplace=True)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset structure. Unable to aggregate.\")\n",
    "\n",
    "    return df_combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bdd382-6058-453c-a951-d3b6b031b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the aggregation function to each dataset\n",
    "df_cpu_sum = aggregate_data(df_cpu)\n",
    "df_disk_sum = aggregate_data(df_disk)\n",
    "df_memory_sum = aggregate_data(df_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee0187d-8897-4b4d-ba32-6e33fe041600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print head(10) and tail(10) for each aggregated dataframe\n",
    "print(\"\\nCPU Aggregated Data:\")\n",
    "print(df_cpu_sum.head(10))\n",
    "print(df_cpu_sum.tail(10))\n",
    "\n",
    "print(\"\\nDisk Aggregated Data:\")\n",
    "print(df_disk_sum.head(10))\n",
    "print(df_disk_sum.tail(10))\n",
    "\n",
    "print(\"\\nMemory Aggregated Data:\")\n",
    "print(df_memory_sum.head(10))\n",
    "print(df_memory_sum.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d504b",
   "metadata": {},
   "source": [
    "### 5. Merging Data (10 pts):\n",
    "\n",
    "a. **Use the Pandas merge function to do an inner join on 'image_name' for dataframes `df_cpu_sum` and `df_disk_sum` created in the previous step. Store the result in a dataframe named `df_new`**.\n",
    "\n",
    "b. **Further merge `df_new` with `df_memory_sum` using an inner join on 'image_name'**.\n",
    "\n",
    "c. **Show the resulting dataframe in each of the steps above**\n",
    "\n",
    "Please refer to the pandas merge method and its parameters here : https://pandas.pydata.org/docs/reference/api/pandas.merge.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_cpu_sum and df_disk_sum on 'image_name' using an inner join\n",
    "df_new = pd.merge(df_cpu_sum, df_disk_sum, on='image_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e6d66-af7c-4959-9143-bcd234c8ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the merged DataFrame after first merge\n",
    "print(\"Merged CPU and Disk Data (first 10 rows):\")\n",
    "print(df_new.head(10))\n",
    "print(df_new.info())  # Show DataFrame structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322167a-c525-41e0-a4f1-1bcef7b39d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df_new with df_memory_sum on 'image_name' using an inner join\n",
    "df_new = pd.merge(df_new, df_memory_sum, on='image_name', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835c1ed-00c9-404e-8834-06ea2cff4d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final merged DataFrame\n",
    "print(\"\\nFinal Merged Data (first 10 rows):\")\n",
    "print(df_new.head(10))\n",
    "print(df_new.info())  # Show DataFrame structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e67a348",
   "metadata": {},
   "source": [
    "### 6. Filtering Data (30 pts):\n",
    "\n",
    "a. **Filter `df_new` to obtain:**\n",
    "\n",
    "   i. `image_name` that had 'working_set_sum' greater than 200,000. Store the result in a variable named 'high_memory_image'.\n",
    "\n",
    "   ii. `image_name` that had 'thread_sum' greater than 200. Store the result in a variable named 'high_thread_image'.\n",
    "\n",
    "   iii. `image_name` that had 'working_set_sum' greater than 200,000, 'thread_sum' less than 50, and 'total_byte_sum' less than 7,000. Store the result in a variable named 'hi_mem_low_thread_low_io'.\n",
    "   \n",
    "   iv. Show each of the filtered dataframe in separate cells. Feel free to add new cells to this notebook.  \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dffad5-3660-4eca-9d52-b35dd2722848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'working_set_sum' greater than 200,000\n",
    "high_memory_image = df_new[df_new['working_set_sum'] > 200000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de6121-fa03-42d5-a3b3-aa7d3a1d2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'threads_sum' greater than 200\n",
    "high_thread_image = df_new[df_new['threads_sum'] > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1577a033-4047-4c1f-bc4e-9e3017109b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for 'working_set_sum' > 200,000, 'threads_sum' < 50, and 'total_byte_sum' < 7,000\n",
    "hi_mem_low_thread_low_io = df_new[\n",
    "    (df_new['working_set_sum'] > 200000) & \n",
    "    (df_new['threads_sum'] < 50) & \n",
    "    (df_new['total_byte_sum'] < 7000)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48f08e3-1fd7-4baa-8391-8f94b5d916bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results\n",
    "print(\"High Memory Image:\")\n",
    "print(high_memory_image)\n",
    "\n",
    "print(\"\\nHigh Thread Image:\")\n",
    "print(high_thread_image)\n",
    "\n",
    "print(\"\\nHigh Memory, Low Thread, Low IO:\")\n",
    "print(hi_mem_low_thread_low_io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689a423-34ed-4a5f-be52-a52a4d579d35",
   "metadata": {},
   "source": [
    "### Coding Style (5 pts) \n",
    "Although we do not enforce a coding style such as PEP 8 (https://peps.python.org/pep-0008/) , please ensure that you have comments for each of the functions defined. Your code is readable, and includes only the code that is required by the assignment. Please remove any commented code, and experimental code that you may have tried. For each of the questions be sure to show some example rows of the dataframe that was modified or created.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300b6af8-6b20-428a-b763-a4962c5de8bb",
   "metadata": {},
   "source": [
    "## Submission on Gradescope\n",
    "\n",
    "**Gradescope canvas left menu -> Gradescop -> PSET 5: Exploratory Data Analysis**\n",
    "\n",
    "**Submission :**\n",
    "Submit the jupyter notebook, and a pdf version of this notebook.\n",
    "\n",
    "To create a pdf of this notebook :  In your browser open print, and save as pdf. Name the pdf LastNameFirstName_pset5.pdf\n",
    "example: DoeJohn_pset5.pdf\n",
    "\n",
    "Name this jupyter notebook with the same format LastNameFirstName_pset5.ipynb\n",
    "\n",
    "Make sure that your notebook has been run before creating pdf. Any outputs from running the code needs to be clearly visible. We need both .ipynb, and pdf of this notebook to assign you grades. \n",
    "\n",
    "Drop all the files in gradescope under PSET 5: Exploratory Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10836d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
